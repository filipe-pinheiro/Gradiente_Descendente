# Aprendizado de Gradiente Descendente

Repositório contendo materiais e notebooks para estudo dos principais métodos de otimização baseados em gradiente descendente: **Batch Gradient Descent**, **Stochastic Gradient Descent** e **Mini-batch Gradient Descent**. O foco é apresentar conceitos, implementações matemáticas e exemplos práticos para aprendizado de regressão linear simples.

---

## Conteúdo

- **Gradiente Batch**: cálculo e atualização dos parâmetros considerando todo o conjunto de dados.
- **Gradiente Estocástico**: atualização dos parâmetros usando um único exemplo por vez.
- **Mini-batch Gradient**: atualização baseada em pequenos lotes (batches) de dados para equilíbrio entre desempenho e estabilidade.
- Análise da função de custo e visualizações da superfície e trajetória dos parâmetros.
- Implementações em Python usando Jupyter Notebooks (`.ipynb`).

---

## Estrutura do Repositório

/  
├── 1.Gradiente Batch.ipynb  
├── 2.Gradiente Estocástico.ipynb  
├── 3.Gradiente Mini-Batch.ipynb  
├── Data/  
└── README.md  

---

## Como usar

1. Clone o repositório:
   ```bash
   git clone https://github.com/filipe-pinheiro/Gradiente_Descida

2. Instale as dependências necessárias (recomendado usar um ambiente virtual):
   ```bash
   pip install -r requirements.txt

3. Abra os notebooks com Jupyter:
   ```bash
   jupyter notebook

4. Navegue até o notebook desejado e execute as células para estudar os conceitos e visualizar os gráficos.

---

## Sobre Notebooks
Cada Notebook traz:
- Explicação matemática do algoritmo.
- Código passo a passo para implementação.
- Visualizações da função de custo e trajetórias do gradiente.
- Exercícios e testes para validar o aprendizado.

---

## Licença

Este projeto está licenciado sob a Licença MIT — veja o arquivo LICENSE para detalhes.

---

### Contato
Para dúvidas, sugestões ou contribuições, abra uma issue ou envie um e-mail para:

Filipe Pinheiro — filipe.pinheirodc@gmail.com

---

## Referências
- A
- B
- C
